{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n",
      "Data Preprocessed\n",
      "`logits_to_text` function loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 15, 128)           25600     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 345)           176985    \n",
      "=================================================================\n",
      "Total params: 827,225\n",
      "Trainable params: 827,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 103s - loss: 1.4949 - acc: 0.6219 - val_loss: 0.8641 - val_acc: 0.7460\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 102s - loss: 0.7254 - acc: 0.7829 - val_loss: 0.5006 - val_acc: 0.8487\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 102s - loss: 0.4321 - acc: 0.8705 - val_loss: 0.2884 - val_acc: 0.9148\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 102s - loss: 0.2888 - acc: 0.9148 - val_loss: 0.2093 - val_acc: 0.9372\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 102s - loss: 0.2244 - acc: 0.9337 - val_loss: 0.1967 - val_acc: 0.9402\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 101s - loss: 0.1864 - acc: 0.9450 - val_loss: 0.1752 - val_acc: 0.9483\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 101s - loss: 0.1630 - acc: 0.9519 - val_loss: 0.1403 - val_acc: 0.9582\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 101s - loss: 0.1409 - acc: 0.9585 - val_loss: 0.1182 - val_acc: 0.9649\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 101s - loss: 0.1249 - acc: 0.9632 - val_loss: 0.1136 - val_acc: 0.9667\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 101s - loss: 0.1164 - acc: 0.9660 - val_loss: 0.1080 - val_acc: 0.9687\n",
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "# Load English data\n",
    "english_sentences = helper.load_data('small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    tokenized_data=tokenizer.texts_to_sequences(x)\n",
    "    return tokenized_data,tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    padded_sequences=pad_sequences(x,maxlen=length,padding='post')\n",
    "    return padded_sequences\n",
    "tests.test_pad(pad)\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = \\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "\n",
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    model = Sequential()\n",
    "    #Embedding\n",
    "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    #Bidirectional Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    #Adapter to fit 2D output to required GRU 3D input shape\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    #Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='tanh')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    learning_rate = 0.001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "\n",
    "print('`logits_to_text` function loaded.')\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    print(model.summary())\n",
    "    # reduced batch size to 100 and train for 20 epochs\n",
    "    model.fit(x, y, batch_size=100, epochs=10, validation_split=0.2)\n",
    "\n",
    "    model.save('savedtrain_data.h5',overwrite=True)\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
